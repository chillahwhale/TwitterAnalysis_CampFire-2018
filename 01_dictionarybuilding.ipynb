{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T22:20:33.208895Z",
     "start_time": "2020-05-13T22:20:31.543410Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T23:27:18.650340Z",
     "start_time": "2020-05-13T23:27:18.625104Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_count</th>\n",
       "      <th>City</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>username</th>\n",
       "      <th>mentions</th>\n",
       "      <th>rewtweets</th>\n",
       "      <th>replies</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Paradise, CA</td>\n",
       "      <td>1066843491555205120</td>\n",
       "      <td>Lord Farquad Quad Squat Squad @Chico, Californ...</td>\n",
       "      <td>2018-11-25 23:58:04+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>areoandmilk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/areoandmilk/status/1066843...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Paradise, CA</td>\n",
       "      <td>1066842521601400832</td>\n",
       "      <td>winter edition #queenadailypic released. @Soda...</td>\n",
       "      <td>2018-11-25 23:54:12+00:00</td>\n",
       "      <td>#queenadailypic</td>\n",
       "      <td>leenathequeena</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/leenathequeena/status/1066...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Paradise, CA</td>\n",
       "      <td>1066841740060098562</td>\n",
       "      <td>Drinking a Def Leppard Pale by @ElysianBrewing...</td>\n",
       "      <td>2018-11-25 23:51:06+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>geradellsworth</td>\n",
       "      <td>@ElysianBrewing @Golden1Center</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/geradellsworth/status/1066...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Paradise, CA</td>\n",
       "      <td>1066841178782482433</td>\n",
       "      <td>The forgotten. #LimeBike #Reno @Reno, Nevada h...</td>\n",
       "      <td>2018-11-25 23:48:52+00:00</td>\n",
       "      <td>#LimeBike #Reno</td>\n",
       "      <td>alittlegordie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/alittlegordie/status/10668...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Paradise, CA</td>\n",
       "      <td>1066839617196961792</td>\n",
       "      <td>SSCC is United and moving Forward !!! The powe...</td>\n",
       "      <td>2018-11-25 23:42:40+00:00</td>\n",
       "      <td>#heritageoffaith</td>\n",
       "      <td>LesSimmons</td>\n",
       "      <td>@sscc7710</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/LesSimmons/status/10668396...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_count          City                   id  \\\n",
       "0            0  Paradise, CA  1066843491555205120   \n",
       "1            1  Paradise, CA  1066842521601400832   \n",
       "2            2  Paradise, CA  1066841740060098562   \n",
       "3            3  Paradise, CA  1066841178782482433   \n",
       "4            4  Paradise, CA  1066839617196961792   \n",
       "\n",
       "                                          tweet_text  \\\n",
       "0  Lord Farquad Quad Squat Squad @Chico, Californ...   \n",
       "1  winter edition #queenadailypic released. @Soda...   \n",
       "2  Drinking a Def Leppard Pale by @ElysianBrewing...   \n",
       "3  The forgotten. #LimeBike #Reno @Reno, Nevada h...   \n",
       "4  SSCC is United and moving Forward !!! The powe...   \n",
       "\n",
       "                   timestamp          hashtags        username  \\\n",
       "0  2018-11-25 23:58:04+00:00               NaN     areoandmilk   \n",
       "1  2018-11-25 23:54:12+00:00   #queenadailypic  leenathequeena   \n",
       "2  2018-11-25 23:51:06+00:00               NaN  geradellsworth   \n",
       "3  2018-11-25 23:48:52+00:00   #LimeBike #Reno   alittlegordie   \n",
       "4  2018-11-25 23:42:40+00:00  #heritageoffaith      LesSimmons   \n",
       "\n",
       "                         mentions  rewtweets  replies  \\\n",
       "0                             NaN          0        0   \n",
       "1                             NaN          0        0   \n",
       "2  @ElysianBrewing @Golden1Center          0        0   \n",
       "3                             NaN          0        0   \n",
       "4                       @sscc7710          1        0   \n",
       "\n",
       "                                                link  \n",
       "0  https://twitter.com/areoandmilk/status/1066843...  \n",
       "1  https://twitter.com/leenathequeena/status/1066...  \n",
       "2  https://twitter.com/geradellsworth/status/1066...  \n",
       "3  https://twitter.com/alittlegordie/status/10668...  \n",
       "4  https://twitter.com/LesSimmons/status/10668396...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities = pd.read_csv('./datasets/cities_clean.csv')\n",
    "cities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T22:21:40.631117Z",
     "start_time": "2020-05-13T22:21:40.627689Z"
    }
   },
   "outputs": [],
   "source": [
    "# counts of hashtages (strip the #)\n",
    "def count_hashtags(df):\n",
    "    keyterms = {}\n",
    "    for i in df['hashtags'].dropna():\n",
    "        i = i.replace(\"#\", '')\n",
    "        for j in i.split():\n",
    "            if j not in keyterms.keys():\n",
    "                keyterms[j] = 1\n",
    "            else:\n",
    "                keyterms[j] += 1\n",
    "    return keyterms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T22:29:08.469014Z",
     "start_time": "2020-05-13T22:29:08.465018Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_keywords(hashtags): \n",
    "    word_filter = ['fire', 'evac', 'smok', 'burn', 'wild', 'blaz', 'hell', 'department',\n",
    "              'inferno', 'help']\n",
    "    keywords = []\n",
    "    for word in set(hashtags):\n",
    "        for wf in word_filter:\n",
    "            if wf in word.lower():\n",
    "                keywords.append(word)\n",
    "    return list(set(keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T22:27:30.114879Z",
     "start_time": "2020-05-13T22:27:30.109602Z"
    }
   },
   "outputs": [],
   "source": [
    "# check each tweet for keyterms and score them\n",
    "def keyscoring(df, keywords):\n",
    "    keyscore = []\n",
    "    for row in df.index:\n",
    "        keyscore.append(0)\n",
    "        for word in str(df.iloc[row]['tweet_text']).split():\n",
    "            if word.lower().replace('#','') in keywords:\n",
    "                try:\n",
    "                    keyscore[row]+=1\n",
    "                except:\n",
    "                    print(f'fail in row {row}')\n",
    "    df['key_score'] = keyscore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T23:27:56.861400Z",
     "start_time": "2020-05-13T23:27:56.854592Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_locs(df):\n",
    "    locs = []\n",
    "    df_num_rows = df.shape[0]\n",
    "    for row in df.index:\n",
    "        url = df.iloc[row]['link']\n",
    "        res = requests.get(url)\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        spans = soup.find_all('span', {'class' : 'permalink-tweet-geo-text'})\n",
    "        try:\n",
    "            location = spans[0].text.replace(\"from\",'').strip()\n",
    "            if location == 'California, USA':\n",
    "                locs.append(df.iloc[row]['City'])\n",
    "            else:\n",
    "                locs.append(location)\n",
    "        except:\n",
    "            print(f'tweet #{row} has no location info')\n",
    "            locs.append('nolocationfound')\n",
    "        \n",
    "        if row % 25 == 0:\n",
    "            time.sleep(3)\n",
    "        if row % 75 == 0:\n",
    "            print(f'Located {row} out of {df_num_rows} tweets.  {row/df_num_rows:.2%}')\n",
    "    df['from_locations'] = locs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T22:23:54.974822Z",
     "start_time": "2020-05-13T22:23:54.971916Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_url(df):\n",
    "    df['text_nourl'] = [re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', i) for i in df['tweet_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T23:25:32.880857Z",
     "start_time": "2020-05-13T23:25:32.877833Z"
    }
   },
   "outputs": [],
   "source": [
    "def func_master(df):\n",
    "    keywords = build_keywords(list(count_hashtags(df)))\n",
    "    remove_url(df)\n",
    "    keyscoring(df, keywords)\n",
    "    get_locs(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T23:43:45.464500Z",
     "start_time": "2020-05-13T23:28:02.903573Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Located 0 out of 1809 tweets.  0.00%\n",
      "Located 75 out of 1809 tweets.  4.15%\n",
      "Located 150 out of 1809 tweets.  8.29%\n",
      "Located 225 out of 1809 tweets.  12.44%\n",
      "Located 300 out of 1809 tweets.  16.58%\n",
      "tweet #341 has no location info\n",
      "Located 375 out of 1809 tweets.  20.73%\n",
      "Located 450 out of 1809 tweets.  24.88%\n",
      "Located 525 out of 1809 tweets.  29.02%\n",
      "Located 600 out of 1809 tweets.  33.17%\n",
      "Located 675 out of 1809 tweets.  37.31%\n",
      "Located 750 out of 1809 tweets.  41.46%\n",
      "tweet #801 has no location info\n",
      "Located 825 out of 1809 tweets.  45.61%\n",
      "Located 900 out of 1809 tweets.  49.75%\n",
      "Located 975 out of 1809 tweets.  53.90%\n",
      "Located 1050 out of 1809 tweets.  58.04%\n",
      "Located 1125 out of 1809 tweets.  62.19%\n",
      "Located 1200 out of 1809 tweets.  66.33%\n",
      "Located 1275 out of 1809 tweets.  70.48%\n",
      "Located 1350 out of 1809 tweets.  74.63%\n",
      "Located 1425 out of 1809 tweets.  78.77%\n",
      "Located 1500 out of 1809 tweets.  82.92%\n",
      "Located 1575 out of 1809 tweets.  87.06%\n",
      "Located 1650 out of 1809 tweets.  91.21%\n",
      "Located 1725 out of 1809 tweets.  95.36%\n",
      "Located 1800 out of 1809 tweets.  99.50%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_count</th>\n",
       "      <th>City</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>username</th>\n",
       "      <th>mentions</th>\n",
       "      <th>rewtweets</th>\n",
       "      <th>replies</th>\n",
       "      <th>link</th>\n",
       "      <th>text_nourl</th>\n",
       "      <th>key_score</th>\n",
       "      <th>from_locations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Paradise, CA</td>\n",
       "      <td>1066843491555205120</td>\n",
       "      <td>Lord Farquad Quad Squat Squad @Chico, Californ...</td>\n",
       "      <td>2018-11-25 23:58:04+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>areoandmilk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/areoandmilk/status/1066843...</td>\n",
       "      <td>Lord Farquad Quad Squat Squad @Chico, California</td>\n",
       "      <td>0</td>\n",
       "      <td>Chico, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Paradise, CA</td>\n",
       "      <td>1066842521601400832</td>\n",
       "      <td>winter edition #queenadailypic released. @Soda...</td>\n",
       "      <td>2018-11-25 23:54:12+00:00</td>\n",
       "      <td>#queenadailypic</td>\n",
       "      <td>leenathequeena</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/leenathequeena/status/1066...</td>\n",
       "      <td>winter edition #queenadailypic released. @Soda...</td>\n",
       "      <td>0</td>\n",
       "      <td>Paradise, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Paradise, CA</td>\n",
       "      <td>1066841740060098562</td>\n",
       "      <td>Drinking a Def Leppard Pale by @ElysianBrewing...</td>\n",
       "      <td>2018-11-25 23:51:06+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>geradellsworth</td>\n",
       "      <td>@ElysianBrewing @Golden1Center</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/geradellsworth/status/1066...</td>\n",
       "      <td>Drinking a Def Leppard Pale by @ElysianBrewing...</td>\n",
       "      <td>0</td>\n",
       "      <td>Sacramento, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Paradise, CA</td>\n",
       "      <td>1066841178782482433</td>\n",
       "      <td>The forgotten. #LimeBike #Reno @Reno, Nevada h...</td>\n",
       "      <td>2018-11-25 23:48:52+00:00</td>\n",
       "      <td>#LimeBike #Reno</td>\n",
       "      <td>alittlegordie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/alittlegordie/status/10668...</td>\n",
       "      <td>The forgotten. #LimeBike #Reno @Reno, Nevada</td>\n",
       "      <td>0</td>\n",
       "      <td>Reno, NV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Paradise, CA</td>\n",
       "      <td>1066839617196961792</td>\n",
       "      <td>SSCC is United and moving Forward !!! The powe...</td>\n",
       "      <td>2018-11-25 23:42:40+00:00</td>\n",
       "      <td>#heritageoffaith</td>\n",
       "      <td>LesSimmons</td>\n",
       "      <td>@sscc7710</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/LesSimmons/status/10668396...</td>\n",
       "      <td>SSCC is United and moving Forward !!! The powe...</td>\n",
       "      <td>0</td>\n",
       "      <td>Sacramento, CA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_count          City                   id  \\\n",
       "0            0  Paradise, CA  1066843491555205120   \n",
       "1            1  Paradise, CA  1066842521601400832   \n",
       "2            2  Paradise, CA  1066841740060098562   \n",
       "3            3  Paradise, CA  1066841178782482433   \n",
       "4            4  Paradise, CA  1066839617196961792   \n",
       "\n",
       "                                          tweet_text  \\\n",
       "0  Lord Farquad Quad Squat Squad @Chico, Californ...   \n",
       "1  winter edition #queenadailypic released. @Soda...   \n",
       "2  Drinking a Def Leppard Pale by @ElysianBrewing...   \n",
       "3  The forgotten. #LimeBike #Reno @Reno, Nevada h...   \n",
       "4  SSCC is United and moving Forward !!! The powe...   \n",
       "\n",
       "                   timestamp          hashtags        username  \\\n",
       "0  2018-11-25 23:58:04+00:00               NaN     areoandmilk   \n",
       "1  2018-11-25 23:54:12+00:00   #queenadailypic  leenathequeena   \n",
       "2  2018-11-25 23:51:06+00:00               NaN  geradellsworth   \n",
       "3  2018-11-25 23:48:52+00:00   #LimeBike #Reno   alittlegordie   \n",
       "4  2018-11-25 23:42:40+00:00  #heritageoffaith      LesSimmons   \n",
       "\n",
       "                         mentions  rewtweets  replies  \\\n",
       "0                             NaN          0        0   \n",
       "1                             NaN          0        0   \n",
       "2  @ElysianBrewing @Golden1Center          0        0   \n",
       "3                             NaN          0        0   \n",
       "4                       @sscc7710          1        0   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://twitter.com/areoandmilk/status/1066843...   \n",
       "1  https://twitter.com/leenathequeena/status/1066...   \n",
       "2  https://twitter.com/geradellsworth/status/1066...   \n",
       "3  https://twitter.com/alittlegordie/status/10668...   \n",
       "4  https://twitter.com/LesSimmons/status/10668396...   \n",
       "\n",
       "                                          text_nourl  key_score  \\\n",
       "0  Lord Farquad Quad Squat Squad @Chico, California           0   \n",
       "1  winter edition #queenadailypic released. @Soda...          0   \n",
       "2  Drinking a Def Leppard Pale by @ElysianBrewing...          0   \n",
       "3      The forgotten. #LimeBike #Reno @Reno, Nevada           0   \n",
       "4  SSCC is United and moving Forward !!! The powe...          0   \n",
       "\n",
       "   from_locations  \n",
       "0       Chico, CA  \n",
       "1    Paradise, CA  \n",
       "2  Sacramento, CA  \n",
       "3        Reno, NV  \n",
       "4  Sacramento, CA  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_master(cities)\n",
    "cities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T22:28:21.121735Z",
     "start_time": "2020-05-13T22:28:21.116293Z"
    }
   },
   "outputs": [],
   "source": [
    "hashtags = list(count_hashtags(cities))\n",
    "\n",
    "keywords = build_keywords(hashtags)\n",
    "\n",
    "remove_url(cities)\n",
    "\n",
    "keyscoring(cities, keywords)\n",
    "cities['key_score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T22:50:00.032977Z",
     "start_time": "2020-05-13T22:33:26.669701Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet #1 has no location info\n",
      "tweet #28 has no location info\n",
      "tweet #33 has no location info\n",
      "tweet #52 has no location info\n",
      "tweet #54 has no location info\n",
      "tweet #63 has no location info\n",
      "tweet #73 has no location info\n",
      "Located 75 out of 1809 tweets.  4.15%\n",
      "tweet #84 has no location info\n",
      "tweet #100 has no location info\n",
      "tweet #129 has no location info\n",
      "tweet #148 has no location info\n",
      "Located 150 out of 1809 tweets.  8.29%\n",
      "tweet #154 has no location info\n",
      "tweet #164 has no location info\n",
      "tweet #186 has no location info\n",
      "tweet #195 has no location info\n",
      "tweet #215 has no location info\n",
      "tweet #217 has no location info\n",
      "Located 225 out of 1809 tweets.  12.44%\n",
      "tweet #240 has no location info\n",
      "tweet #244 has no location info\n",
      "tweet #267 has no location info\n",
      "tweet #295 has no location info\n",
      "Located 300 out of 1809 tweets.  16.58%\n",
      "tweet #315 has no location info\n",
      "tweet #331 has no location info\n",
      "tweet #334 has no location info\n",
      "tweet #341 has no location info\n",
      "tweet #345 has no location info\n",
      "tweet #347 has no location info\n",
      "tweet #351 has no location info\n",
      "tweet #367 has no location info\n",
      "tweet #372 has no location info\n",
      "Located 375 out of 1809 tweets.  20.73%\n",
      "tweet #376 has no location info\n",
      "tweet #391 has no location info\n",
      "tweet #399 has no location info\n",
      "tweet #410 has no location info\n",
      "tweet #416 has no location info\n",
      "tweet #418 has no location info\n",
      "tweet #432 has no location info\n",
      "tweet #438 has no location info\n",
      "tweet #445 has no location info\n",
      "Located 450 out of 1809 tweets.  24.88%\n",
      "tweet #454 has no location info\n",
      "tweet #464 has no location info\n",
      "tweet #467 has no location info\n",
      "tweet #484 has no location info\n",
      "tweet #487 has no location info\n",
      "tweet #499 has no location info\n",
      "tweet #504 has no location info\n",
      "tweet #522 has no location info\n",
      "Located 525 out of 1809 tweets.  29.02%\n",
      "tweet #539 has no location info\n",
      "tweet #541 has no location info\n",
      "tweet #544 has no location info\n",
      "tweet #547 has no location info\n",
      "tweet #550 has no location info\n",
      "tweet #552 has no location info\n",
      "tweet #556 has no location info\n",
      "tweet #558 has no location info\n",
      "tweet #559 has no location info\n",
      "tweet #587 has no location info\n",
      "tweet #591 has no location info\n",
      "tweet #592 has no location info\n",
      "tweet #593 has no location info\n",
      "tweet #595 has no location info\n",
      "tweet #597 has no location info\n",
      "tweet #598 has no location info\n",
      "Located 600 out of 1809 tweets.  33.17%\n",
      "tweet #606 has no location info\n",
      "tweet #611 has no location info\n",
      "tweet #612 has no location info\n",
      "tweet #619 has no location info\n",
      "tweet #622 has no location info\n",
      "tweet #647 has no location info\n",
      "tweet #649 has no location info\n",
      "tweet #650 has no location info\n",
      "tweet #652 has no location info\n",
      "tweet #653 has no location info\n",
      "tweet #654 has no location info\n",
      "tweet #655 has no location info\n",
      "tweet #657 has no location info\n",
      "tweet #659 has no location info\n",
      "tweet #672 has no location info\n",
      "tweet #674 has no location info\n",
      "Located 675 out of 1809 tweets.  37.31%\n",
      "tweet #690 has no location info\n",
      "tweet #702 has no location info\n",
      "tweet #726 has no location info\n",
      "tweet #728 has no location info\n",
      "tweet #735 has no location info\n",
      "tweet #741 has no location info\n",
      "tweet #742 has no location info\n",
      "tweet #746 has no location info\n",
      "tweet #749 has no location info\n",
      "Located 750 out of 1809 tweets.  41.46%\n",
      "tweet #761 has no location info\n",
      "tweet #763 has no location info\n",
      "tweet #788 has no location info\n",
      "tweet #801 has no location info\n",
      "tweet #820 has no location info\n",
      "Located 825 out of 1809 tweets.  45.61%\n",
      "tweet #829 has no location info\n",
      "tweet #830 has no location info\n",
      "tweet #832 has no location info\n",
      "tweet #849 has no location info\n",
      "tweet #854 has no location info\n",
      "tweet #855 has no location info\n",
      "tweet #856 has no location info\n",
      "tweet #861 has no location info\n",
      "tweet #862 has no location info\n",
      "tweet #869 has no location info\n",
      "tweet #873 has no location info\n",
      "tweet #874 has no location info\n",
      "tweet #875 has no location info\n",
      "tweet #878 has no location info\n",
      "tweet #880 has no location info\n",
      "tweet #883 has no location info\n",
      "tweet #885 has no location info\n",
      "tweet #888 has no location info\n",
      "tweet #890 has no location info\n",
      "tweet #892 has no location info\n",
      "tweet #893 has no location info\n",
      "tweet #896 has no location info\n",
      "Located 900 out of 1809 tweets.  49.75%\n",
      "tweet #903 has no location info\n",
      "tweet #905 has no location info\n",
      "tweet #907 has no location info\n",
      "tweet #911 has no location info\n",
      "tweet #913 has no location info\n",
      "tweet #915 has no location info\n",
      "tweet #919 has no location info\n",
      "tweet #920 has no location info\n",
      "tweet #921 has no location info\n",
      "tweet #922 has no location info\n",
      "tweet #923 has no location info\n",
      "tweet #924 has no location info\n",
      "tweet #925 has no location info\n",
      "tweet #936 has no location info\n",
      "tweet #937 has no location info\n",
      "tweet #949 has no location info\n",
      "tweet #959 has no location info\n",
      "tweet #970 has no location info\n",
      "tweet #972 has no location info\n",
      "tweet #973 has no location info\n",
      "Located 975 out of 1809 tweets.  53.90%\n",
      "tweet #982 has no location info\n",
      "tweet #992 has no location info\n",
      "tweet #997 has no location info\n",
      "tweet #1005 has no location info\n",
      "tweet #1021 has no location info\n",
      "Located 1050 out of 1809 tweets.  58.04%\n",
      "tweet #1050 has no location info\n",
      "tweet #1072 has no location info\n",
      "tweet #1088 has no location info\n",
      "tweet #1101 has no location info\n",
      "tweet #1113 has no location info\n",
      "tweet #1122 has no location info\n",
      "Located 1125 out of 1809 tweets.  62.19%\n",
      "tweet #1140 has no location info\n",
      "tweet #1141 has no location info\n",
      "tweet #1149 has no location info\n",
      "tweet #1168 has no location info\n",
      "tweet #1173 has no location info\n",
      "tweet #1187 has no location info\n",
      "tweet #1194 has no location info\n",
      "Located 1200 out of 1809 tweets.  66.33%\n",
      "tweet #1202 has no location info\n",
      "tweet #1208 has no location info\n",
      "tweet #1226 has no location info\n",
      "tweet #1233 has no location info\n",
      "tweet #1234 has no location info\n",
      "tweet #1251 has no location info\n",
      "tweet #1265 has no location info\n",
      "tweet #1273 has no location info\n",
      "tweet #1274 has no location info\n",
      "Located 1275 out of 1809 tweets.  70.48%\n",
      "tweet #1275 has no location info\n",
      "tweet #1278 has no location info\n",
      "tweet #1279 has no location info\n",
      "tweet #1290 has no location info\n",
      "tweet #1292 has no location info\n",
      "tweet #1296 has no location info\n",
      "tweet #1303 has no location info\n",
      "tweet #1315 has no location info\n",
      "tweet #1320 has no location info\n",
      "tweet #1324 has no location info\n",
      "tweet #1325 has no location info\n",
      "tweet #1342 has no location info\n",
      "Located 1350 out of 1809 tweets.  74.63%\n",
      "tweet #1350 has no location info\n",
      "tweet #1351 has no location info\n",
      "tweet #1363 has no location info\n",
      "tweet #1369 has no location info\n",
      "tweet #1371 has no location info\n",
      "tweet #1372 has no location info\n",
      "tweet #1387 has no location info\n",
      "tweet #1393 has no location info\n",
      "tweet #1400 has no location info\n",
      "tweet #1402 has no location info\n",
      "tweet #1410 has no location info\n",
      "tweet #1413 has no location info\n",
      "tweet #1420 has no location info\n",
      "tweet #1423 has no location info\n",
      "Located 1425 out of 1809 tweets.  78.77%\n",
      "tweet #1427 has no location info\n",
      "tweet #1434 has no location info\n",
      "tweet #1441 has no location info\n",
      "tweet #1444 has no location info\n",
      "tweet #1456 has no location info\n",
      "tweet #1461 has no location info\n",
      "tweet #1480 has no location info\n",
      "tweet #1497 has no location info\n",
      "tweet #1499 has no location info\n",
      "Located 1500 out of 1809 tweets.  82.92%\n",
      "tweet #1502 has no location info\n",
      "tweet #1505 has no location info\n",
      "tweet #1508 has no location info\n",
      "tweet #1510 has no location info\n",
      "tweet #1513 has no location info\n",
      "tweet #1515 has no location info\n",
      "tweet #1516 has no location info\n",
      "tweet #1543 has no location info\n",
      "tweet #1547 has no location info\n",
      "tweet #1548 has no location info\n",
      "tweet #1549 has no location info\n",
      "tweet #1551 has no location info\n",
      "tweet #1553 has no location info\n",
      "tweet #1554 has no location info\n",
      "tweet #1561 has no location info\n",
      "tweet #1563 has no location info\n",
      "tweet #1568 has no location info\n",
      "tweet #1569 has no location info\n",
      "Located 1575 out of 1809 tweets.  87.06%\n",
      "tweet #1576 has no location info\n",
      "tweet #1579 has no location info\n",
      "tweet #1609 has no location info\n",
      "tweet #1610 has no location info\n",
      "tweet #1612 has no location info\n",
      "tweet #1613 has no location info\n",
      "tweet #1615 has no location info\n",
      "tweet #1616 has no location info\n",
      "tweet #1617 has no location info\n",
      "tweet #1619 has no location info\n",
      "tweet #1620 has no location info\n",
      "tweet #1622 has no location info\n",
      "tweet #1635 has no location info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet #1637 has no location info\n",
      "tweet #1639 has no location info\n",
      "Located 1650 out of 1809 tweets.  91.21%\n",
      "tweet #1689 has no location info\n",
      "tweet #1690 has no location info\n",
      "tweet #1705 has no location info\n",
      "tweet #1706 has no location info\n",
      "tweet #1712 has no location info\n",
      "Located 1725 out of 1809 tweets.  95.36%\n",
      "tweet #1725 has no location info\n",
      "tweet #1727 has no location info\n",
      "tweet #1729 has no location info\n",
      "tweet #1739 has no location info\n",
      "tweet #1758 has no location info\n",
      "tweet #1768 has no location info\n",
      "tweet #1773 has no location info\n",
      "tweet #1789 has no location info\n",
      "tweet #1793 has no location info\n",
      "tweet #1795 has no location info\n",
      "tweet #1798 has no location info\n",
      "tweet #1799 has no location info\n",
      "Located 1800 out of 1809 tweets.  99.50%\n",
      "tweet #1800 has no location info\n",
      "tweet #1801 has no location info\n",
      "tweet #1806 has no location info\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Chico, CA             791\n",
       "Paradise, CA          576\n",
       "nolocationfound       250\n",
       "Oroville, CA          112\n",
       "Magalia, CA            17\n",
       "Durham, CA             12\n",
       "Gridley, CA            12\n",
       "South Oroville, CA      9\n",
       "Thermalito, CA          8\n",
       "Sacramento, CA          8\n",
       "Oroville East, CA       3\n",
       "Folsom, CA              2\n",
       "Glen Ellen, CA          1\n",
       "Reno, NV                1\n",
       "Yountville, CA          1\n",
       "Napa, CA                1\n",
       "Linda, CA               1\n",
       "Nevada City, CA         1\n",
       "Redding, CA             1\n",
       "Rocklin, CA             1\n",
       "Florin, CA              1\n",
       "Name: from_locations, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_locs(cities)\n",
    "cities['from_locations'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T23:11:53.264578Z",
     "start_time": "2020-05-12T23:11:53.261947Z"
    }
   },
   "outputs": [],
   "source": [
    "#  pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Currently unused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T19:14:10.240601Z",
     "start_time": "2020-05-12T19:12:47.134373Z"
    }
   },
   "outputs": [],
   "source": [
    "# use hashtag list to build dictionary of keyterms to use combined with that of the gensim model\n",
    "# Import word vectors into \"model.\"\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('../lexvec.enwiki+newscrawl.300d.W.pos.vectors')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T19:14:11.548581Z",
     "start_time": "2020-05-12T19:14:10.244091Z"
    }
   },
   "outputs": [],
   "source": [
    "keyt = []\n",
    "keywords = ['fire', 'smoke', 'wildfire', 'campfire', 'forest', 'evacuate', 'hell']\n",
    "for word in keywords:\n",
    "    kt = [x[0] for x in model.most_similar(word, topn = 25)]\n",
    "    keyt+= kt\n",
    "word_list = list(set(keyt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T19:14:11.578293Z",
     "start_time": "2020-05-12T19:14:11.552298Z"
    }
   },
   "outputs": [],
   "source": [
    "important_words = []\n",
    "for word in keyterms.keys():\n",
    "    if word in word_list:\n",
    "        important_words.append(word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
